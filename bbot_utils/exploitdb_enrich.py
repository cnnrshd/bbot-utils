"""Reads an NDJSON file, extracts CVE ID, and queries a local ExploitDB database for POC Exploits"""

import argparse
import json
import logging
from functools import lru_cache
from pathlib import Path
from sys import stderr, stdin, stdout

import httpx
import pandas as pd
import tqdm

from bbot_utils.common import logger as base_logger

logger = base_logger.getChild("exploitdb_enrich")

APP_DATA_PATH = Path.home() / ".bbot_utils" / "exploitdb_enrich"
APP_DATA_PATH.mkdir(parents=True, exist_ok=True)
DB_PATH = APP_DATA_PATH / "files_exploits.json"
HASH_PATH = APP_DATA_PATH / "files_exploits.json.hash"
DB_URL = "https://gitlab.com/api/v4/projects/40927511/repository/files/files_exploits.csv/raw?ref=main"

# There is no benefit to using async, but every other script in this repo uses it
client = httpx.AsyncClient()


def local_db_exists() -> bool:
    """Checks if the local ExploitDB database exists"""
    return DB_PATH.exists()


def local_hash() -> str:
    """Returns the stored hash from when the DB was last downloaded"""
    if HASH_PATH.exists():
        with open(HASH_PATH, "r") as f:
            return f.read().strip()
    else:
        return ""


async def gitlab_hash() -> str:
    resp = await client.head(DB_URL)
    if resp.status_code == 200:
        return resp.headers["X-Gitlab-Content-Sha256"]


async def up_to_date_db() -> bool:
    """Checks if the local ExploitDB database is up to date"""
    _logger = logger.getChild("up_to_date_db")
    hash_l = local_hash()
    if hash_l == "":
        _logger.info("No local ExploitDB database found")
        return False
    hash_r: str = await gitlab_hash()
    return hash_l == hash_r


async def download_db() -> bool:
    """Downloads the ExploitDB database and updates local hash"""
    _logger = logger.getChild("download_db")
    _logger.info("Downloading ExploitDB database")
    try:
        # Stream the file to disk
        # resp = await client.get(url=DB_URL)
        # if resp.status_code != 200:
        #     _logger.error(
        #         f"Unexpected response from Gitlab: {resp.status_code}\t{resp.text}"
        #     )
        #     return False
        pd.read_csv(DB_URL).to_json(DB_PATH)
        # Get the hash
        hash_r = await gitlab_hash()
        with open(HASH_PATH, "w") as f:
            f.write(hash_r)
        return True
    except Exception as e:
        _logger.error(f"Error downloading ExploitDB database: {e}")
        return False


def query_db(search: str, df: pd.DataFrame) -> list[dict]:
    """Queries the provided db with the provided search string against all columns, returning all rows that match as a list of dicts"""
    _logger = logger.getChild("query_db")
    _logger.info(f"Querying ExploitDB database for {search}")
    try:
        return df[df.isin([search]).any(axis=1)].to_dict("records")
    except Exception as e:
        _logger.error(f"Error querying ExploitDB database: {e}")
        return []


def parse_args():
    parser = argparse.ArgumentParser(
        description="Downloads a local copy of the ExploitDB Database and queries it for POC exploits"
    )

    parser.add_argument(
        "-i",
        "--input-file",
        type=argparse.FileType("r"),
        default=stdin,
        help="File to read input from",
    )
    parser.add_argument(
        "-o",
        "--output-file",
        type=argparse.FileType("w"),
        default=stdout,
        help="File to write output to",
    )
    parser.add_argument(
        "--search-key",
        default="cve",
        help="Key to use for searching the ExploitDB database. Defaults to 'cve'.",
        type=str,
    )
    logger_group = parser.add_argument_group("Logging")
    logger_group.add_argument(
        "--debug", action="store_true", help="Enable debug logging.", default=False
    )
    quiet_group = logger_group.add_mutually_exclusive_group(required=False)
    quiet_group.add_argument(
        "--quiet",
        action="store_true",
        dest="quiet",
        help="Set logging level to WARNING.",
    )
    quiet_group.add_argument(
        "--no-quiet",
        action="store_false",
        dest="quiet",
        help="Set logging level to INFO.",
    )
    parser.set_defaults(quiet=False)
    progress_group = parser.add_mutually_exclusive_group(required=False)
    progress_group.add_argument(
        "--no-progress",
        action="store_false",
        help="Remove progress bar.",
        default=False,
        dest="progress",
    )
    progress_group.add_argument(
        "--progress",
        action="store_true",
        help="Add progress bar to stderr. Recommend to send output to a file, otherwise the progress bar will be reprinted with each update.",
        default=False,
        dest="progress",
    )
    parser.set_defaults(progress=False)
    parser.add_argument(
        "--enrichment-keys",
        nargs="+",
        default=["description", "verified"],
        help="Keys to add to the output JSON. Use 'all' for all keys.",
        type=str,
    )
    parser.add_argument(
        "--target-key",
        default="exploitdb",
        help="Key to add the enrichment data to",
        type=str,
    )
    args = parser.parse_args()
    return args


async def main():
    args = parse_args()
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    logger.debug(f"{args=}")
    # Do some quick validation
    if not await up_to_date_db():
        logger.info("Local ExploitDB database out of date, downloading")
        await download_db()
    if not local_db_exists():
        logger.info("No local ExploitDB database found, downloading")
        await download_db()
    input_data = [json.loads(line) for line in args.input_file]
    logger.info(f"Enriching {len(input_data)} records")
    for data in tqdm.tqdm(
        input_data,
        file=stderr,
        disable=not args.progress,
        desc="Performing ExploitDB Enrichment",
    ):
        try:
            search = data[args.search_key]
            results = query_db(search, pd.read_json(DB_PATH))
            enrich = []
            if args.enrichment_keys == ["all"]:
                enrich = results
            else:
                enrich = [{k: r[k] for k in args.enrichment_keys} for r in results]
            data[args.target_key] = enrich
            args.output_file.write(json.dumps(data) + "\n")
        except Exception as e:
            logger.error(f"Error processing line: {e}")


def run():
    import asyncio

    asyncio.run(main())


if __name__ == "__main__":
    run()
